Path: news.gmane.org!not-for-mail
From: Peter Zijlstra <a.p.zijlstra@chello.nl>
Newsgroups: gmane.linux.kernel
Subject: [PATCH 19/19] mm, numa: retry failed page migrations
Date: Tue, 31 Jul 2012 21:12:23 +0200
Lines: 124
Approved: news@gmane.org
Message-ID: <20120731192809.489683051@chello.nl>
References: <20120731191204.540691987@chello.nl>
NNTP-Posting-Host: plane.gmane.org
X-Trace: dough.gmane.org 1343764141 3841 80.91.229.3 (31 Jul 2012 19:49:01 GMT)
X-Complaints-To: usenet@dough.gmane.org
NNTP-Posting-Date: Tue, 31 Jul 2012 19:49:01 +0000 (UTC)
Cc: linux-kernel@vger.kernel.org,
	Peter Zijlstra <a.p.zijlstra@chello.nl>
To: mingo@kernel.org, riel@redhat.com, oleg@redhat.com, pjt@google.com,
	akpm@linux-foundation.org, torvalds@linux-foundation.org,
	tglx@linutronix.de, Lee.Schermerhorn@hp.com
Original-X-From: linux-kernel-owner@vger.kernel.org Tue Jul 31 21:48:56 2012
Return-path: <linux-kernel-owner@vger.kernel.org>
Envelope-to: glk-linux-kernel-3@plane.gmane.org
Original-Received: from vger.kernel.org ([209.132.180.67])
	by plane.gmane.org with esmtp (Exim 4.69)
	(envelope-from <linux-kernel-owner@vger.kernel.org>)
	id 1SwIR2-0003mm-NP
	for glk-linux-kernel-3@plane.gmane.org; Tue, 31 Jul 2012 21:48:53 +0200
Original-Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1756250Ab2GaTsr (ORCPT <rfc822;glk-linux-kernel-3@m.gmane.org>);
	Tue, 31 Jul 2012 15:48:47 -0400
Original-Received: from casper.infradead.org ([85.118.1.10]:58924 "EHLO
	casper.infradead.org" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1753399Ab2GaTom (ORCPT
	<rfc822;linux-kernel@vger.kernel.org>);
	Tue, 31 Jul 2012 15:44:42 -0400
Original-Received: from dhcp-089-099-019-018.chello.nl ([89.99.19.18] helo=twins)
	by casper.infradead.org with esmtpsa (Exim 4.76 #1 (Red Hat Linux))
	id 1SwIMr-0007v4-9f; Tue, 31 Jul 2012 19:44:33 +0000
Original-Received: by twins (Postfix, from userid 0)
	id A31358483D97; Tue, 31 Jul 2012 21:44:28 +0200 (CEST)
User-Agent: quilt/0.48-1
Content-Disposition: inline; filename=rik_van_riel-mm_numa-retry_failed_page_migrations.patch
Original-Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
Xref: news.gmane.org gmane.linux.kernel:1335987
Archived-At: <http://permalink.gmane.org/gmane.linux.kernel/1335987>

From: Rik van Riel <riel@redhat.com>

Keep track of how many NUMA page migrations succeeded and
failed (in a way that wants retrying later) per process.

If a lot of the page migrations of a process fail, unmap the
process pages some point later, so the migration can be tried
again at the next fault.

Signed-off-by: Rik van Riel <riel@redhat.com>
Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
---
 include/linux/mm_types.h |    2 ++
 kernel/sched/core.c      |    2 ++
 kernel/sched/fair.c      |   19 ++++++++++++++++++-
 mm/memory.c              |   15 ++++++++++++---
 4 files changed, 34 insertions(+), 4 deletions(-)
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -397,6 +397,8 @@ struct mm_struct {
 #ifdef CONFIG_NUMA
 	unsigned int  numa_big;
 	unsigned long numa_next_scan;
+	unsigned int  numa_migrate_success;
+	unsigned int  numa_migrate_failed;
 #endif
 	struct uprobes_state uprobes_state;
 };
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -1727,6 +1727,8 @@ static void __sched_fork(struct task_str
 	if (p->mm && atomic_read(&p->mm->mm_users) == 1) {
 		p->mm->numa_big = 0;
 		p->mm->numa_next_scan = jiffies;
+		p->mm->numa_migrate_success = 0;
+		p->mm->numa_migrate_failed = 0;
 	}
 
 	p->node = -1;
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -857,6 +857,18 @@ static bool task_numa_big(struct task_st
 	return runtime > walltime * max(1, weight / 2);
 }
 
+static bool many_migrate_failures(struct task_struct *p)
+{
+	if (!p->mm)
+		return false;
+
+	/* More than 1/4 of the attempted NUMA page migrations failed. */
+	if (p->mm->numa_migrate_failed * 3 > p->mm->numa_migrate_success)
+		return true;
+
+	return false;
+}
+
 /*
  * The expensive part of numa migration is done from task_work context.
  */
@@ -909,6 +921,10 @@ void task_numa_work(struct task_work *wo
 		rcu_read_unlock();
 	}
 
+	/* Age the numa migrate statistics. */
+	p->mm->numa_migrate_failed /= 2;
+	p->mm->numa_migrate_success /= 2;
+
 	/*
 	 * Trigger fault driven migration, small processes do direct
 	 * lazy migration, big processes do gradual task<->page relations.
@@ -962,7 +978,8 @@ void task_tick_numa(struct rq *rq, struc
 		 * keep the task<->page map accurate.
 		 */
 		if (curr->node_last == node &&
-		    (curr->node != node || curr->mm->numa_big)) {
+		    (curr->node != node || curr->mm->numa_big ||
+				many_migrate_failures(curr))) {
 			/*
 			 * We can re-use curr->rcu because we checked curr->mm
 			 * != NULL so release_task()->call_rcu() was not called
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -3452,7 +3452,7 @@ static int do_prot_none(struct mm_struct
 {
 	struct page *page = NULL;
 	spinlock_t *ptl;
-	int node;
+	int node, ret;
 
 	ptl = pte_lockptr(mm, pmd);
 	spin_lock(ptl);
@@ -3472,18 +3472,27 @@ static int do_prot_none(struct mm_struct
 	pte_unmap_unlock(ptep, ptl);
 
 	node = mpol_misplaced(page, vma, address, mm->numa_big);
-	if (node == -1)
+	if (node == -1) {
+		mm->numa_migrate_success++;
 		goto do_fixup;
+	}
 
 	/*
 	 * Page migration will install a new pte with vma->vm_page_prot,
 	 * otherwise fall-through to the fixup. Next time,.. perhaps.
 	 */
-	if (!migrate_misplaced_page(mm, page, node)) {
+	ret = migrate_misplaced_page(mm, page, node);
+	if (!ret) {
+		mm->numa_migrate_success++;
 		put_page(page);
 		return 0;
 	}
 
+	if (ret == -ENOMEM || ret == -EBUSY) {
+		/* This fault should be tried again later. */
+		mm->numa_migrate_failed++;
+	}
+
 do_fixup:
 	/*
 	 * OK, nothing to do,.. change the protection back to what it


