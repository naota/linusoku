Path: news.gmane.org!not-for-mail
From: "S, Venkatraman" <svenkatr@ti.com>
Newsgroups: gmane.linux.ports.arm.msm,gmane.linux.kernel.mmc,gmane.linux.kernel
Subject: Re: [PATCH v2 1/1] mmc: block: Add write packing control
Date: Thu, 26 Jul 2012 20:58:24 +0530
Lines: 143
Approved: news@gmane.org
Message-ID: <CANfBPZ9NZAdK4tEDJ8wiNKNjRUuqaLKk3qqVZVgod8vwYVuOYw@mail.gmail.com>
References: <34e29d774773a7dfa3e1b57232249b68.squirrel@www.codeaurora.org>
NNTP-Posting-Host: plane.gmane.org
Mime-Version: 1.0
Content-Type: text/plain; charset=ISO-8859-1
X-Trace: dough.gmane.org 1343316534 8971 80.91.229.3 (26 Jul 2012 15:28:54 GMT)
X-Complaints-To: usenet@dough.gmane.org
NNTP-Posting-Date: Thu, 26 Jul 2012 15:28:54 +0000 (UTC)
Cc: Chris Ball <cjb@laptop.org>, Muthu Kumar <muthu.lkml@gmail.com>,
	linux-mmc@vger.kernel.org, linux-arm-msm@vger.kernel.org,
	open list <linux-kernel@vger.kernel.org>,
	Seungwon Jeon <tgih.jun@samsung.com>
To: merez@codeaurora.org
Original-X-From: linux-arm-msm-owner@vger.kernel.org Thu Jul 26 17:28:51 2012
Return-path: <linux-arm-msm-owner@vger.kernel.org>
Envelope-to: glpam-linux-arm-msm@plane.gmane.org
Original-Received: from vger.kernel.org ([209.132.180.67])
	by plane.gmane.org with esmtp (Exim 4.69)
	(envelope-from <linux-arm-msm-owner@vger.kernel.org>)
	id 1SuPzc-00037O-Mv
	for glpam-linux-arm-msm@plane.gmane.org; Thu, 26 Jul 2012 17:28:49 +0200
Original-Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752276Ab2GZP2r (ORCPT <rfc822;glpam-linux-arm-msm@m.gmane.org>);
	Thu, 26 Jul 2012 11:28:47 -0400
Original-Received: from na3sys009aog138.obsmtp.com ([74.125.149.19]:47077 "EHLO
	na3sys009aog138.obsmtp.com" rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1752148Ab2GZP2q (ORCPT
	<rfc822;linux-arm-msm@vger.kernel.org>);
	Thu, 26 Jul 2012 11:28:46 -0400
Original-Received: from mail-ob0-f179.google.com ([209.85.214.179]) (using TLSv1) by na3sys009aob138.postini.com ([74.125.148.12]) with SMTP
	ID DSNKUBFiLQjLNcqIH2gMurolyhl9LmZcMZV5@postini.com; Thu, 26 Jul 2012 08:28:46 PDT
Original-Received: by obbeh20 with SMTP id eh20so3029238obb.24
        for <linux-arm-msm@vger.kernel.org>; Thu, 26 Jul 2012 08:28:45 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=google.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type:x-gm-message-state;
        bh=4fiDRWazBw5Sxng/GN+qlS2ot6yswJIYo82Gq6OVRSY=;
        b=UReWLN7FbfnxISbJ3n205IUdgUOV3PXUBExOF1BU8FgZyUUUWq523s8rVIsYweEGQj
         y6LPAMSE8hhnLAvJXr3H1vB7lJ8GNslhssWJKh5UMaqdNBL2+hTs61OUlODN4eQYPg7p
         KTo4ab8mYhnBZ2JPJMzud1Gop+xh0U564qZlbbuTp8KnXuCBMm1rEvaYPwX+ojeAVXJk
         dr5kp5LnxH/U5kxR9XA3ks6HtPE1vacfWwuRvD9y15xgllbhmjZzZEYm4iHwRVM9iOt3
         W3X8B8l59nUK7aGn95u7munek/U5TJ5KBgB0rZ1NQaXg+n5WfNHfZP4b8YGhZRYEnEHu
         66aA==
Original-Received: by 10.182.89.102 with SMTP id bn6mr41952501obb.7.1343316525215; Thu,
 26 Jul 2012 08:28:45 -0700 (PDT)
Original-Received: by 10.182.67.197 with HTTP; Thu, 26 Jul 2012 08:28:24 -0700 (PDT)
In-Reply-To: <34e29d774773a7dfa3e1b57232249b68.squirrel@www.codeaurora.org>
X-Gm-Message-State: ALoCoQkLS/gacfLWuJRg1CzeiyOHVR6zzgFT1I+NcXWcjpT7bIMWAnS1mL/NH4X+aaCx7Txn01w6
Original-Sender: linux-arm-msm-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-arm-msm.vger.kernel.org>
X-Mailing-List: linux-arm-msm@vger.kernel.org
Xref: news.gmane.org gmane.linux.ports.arm.msm:2888 gmane.linux.kernel.mmc:15758 gmane.linux.kernel:1333730
Archived-At: <http://permalink.gmane.org/gmane.linux.kernel/1333730>

On Tue, Jul 24, 2012 at 2:14 PM,  <merez@codeaurora.org> wrote:
> On Mon, July 23, 2012 5:22 am, S, Venkatraman wrote:
>> On Mon, Jul 23, 2012 at 5:13 PM,  <merez@codeaurora.org> wrote:
>>> On Wed, July 18, 2012 12:26 am, Chris Ball wrote:
>>>> Hi,  [removing Jens and the documentation list, since now we're
> talking about the MMC side only]
>>>> On Wed, Jul 18 2012, merez@codeaurora.org wrote:
>>>>> Is there anything else that holds this patch from being pushed to
>>> mmc-next?
>>>> Yes, I'm still uncomfortable with the write packing patchsets for a
>>> couple of reasons, and I suspect that the sum of those reasons means that
>>> we should probably plan on holding off merging it until after 3.6.
>>>> Here are the open issues; please correct any misunderstandings: With
> Seungwon's patchset ("Support packed write command"):
>>>> * I still don't have a good set of representative benchmarks showing
>>>>   what kind of performance changes come with this patchset.  It seems
>>> like we've had a small amount of testing on one controller/eMMC part combo
>>> from Seungwon, and an entirely different test from Maya, and the
> results
>>> aren't documented fully anywhere to the level of describing what the
> hardware was, what the test was, and what the results were before and
> after the patchset.
>>> Currently, there is only one card vendor that supports packed commands.
> Following are our sequential write (LMDD) test results on 2 of our
> targets
>>> (in MB/s):
>>>                        No packing        packing
>>> Target 1 (SDR 50MHz)     15               25
>>> Target 2 (DDR 50MHz)     20               30
>>>> With the reads-during-writes regression:
>>>> * Venkat still has open questions about the nature of the read
>>>>   regression, and thinks we should understand it with blktrace before
>>> trying to fix it.  Maya has a theory about writes overwhelming reads, but
>>> Venkat doesn't understand why this would explain the observed
>>> bandwidth drop.
>>> The degradation of read due to writes is not a new behavior and exists
> also without the write packing feature (which only increases the
> degradation). Our investigation of this phenomenon led us to the
> Conclusion that a new scheduling policy should be used for mobile
> devices,
>>> but this is not related to the current discussion of the write packing
> feature.
>>> The write packing feature increases the degradation of read due to
> write
>>> since it allows the MMC to fetch many write requests in a row, instead of
>>> fetching only one at a time.  Therefore some of the read requests will
> have to wait for the completion of more write requests before they can
> be
>>> issued.
>>
>> I am a bit puzzled by this claim. One thing I checked carefully when
> reviewing write packing patches from SJeon was that the code didn't
> plough through a mixed list of reads and writes and selected only
> writes.
>> This section of the code in "mmc_blk_prep_packed_list()", from v8
> patchset..
>> <Quote>
>> +               if (rq_data_dir(cur) != rq_data_dir(next)) {
>> +                       put_back = 1;
>> +                       break;
>> +               }
>> </Quote>
>>
>> means that once a read is encountered in the middle of write packing,
> the packing is stopped at that point and it is executed. Then the next
> blk_fetch_request should get the next read and continue as before.
>>
>> IOW, the ordering of reads and writes is _not_ altered when using packed
> commands.
>> For example if there were 5 write requests, followed by 1 read,
>> followed by 5 more write requests in the request_queue, the first 5
> writes will be executed as one "packed command", then the read will be
> executed, and then the remaining 5 writes will be executed as one
> "packed command". So the read does not have to wait any more than it
> waited before (packing feature)
>
> Let me try to better explain with your example.
> Without packing the MMC layer will fetch 2 write requests and wait for the
> first write request completion before fetching another write request.
> During this time the read request could be inserted into the CFQ and since
> it has higher priority than the async write it will be dispatched in the
> next fetch. So, the result would be 2 write requests followed by one read
> request and the read would have to wait for completion of only 2 write
> requests.
> With packing, all the 5 write requests will be fetched in a row, and then
> the read will arrive and be dispatched in the next fetch. Then the read
> will have to wait for the completion of 5 write requests.
>
> Few more clarifications:
> Due to the plug list mechanism in the block layer the applications can
> "aggregate" several requests to be inserted into the scheduler before
> waking the MMC queue thread.
> This leads to a situation where there are several write requests in the
> CFQ queue when MMC starts to do the fetches.
>
> If the read was inserted while we are building the packed command then I
> agree that we should have seen less effect on the read performance.
> However, the write packing statistics show that in most of the cases the
> packing stopped due to an empty queue, meaning that the read was inserted
> to the CFQ after all the pending write requests were fetched and packed.
>
> Following is an example for write packing statistics of a READ/WRITE
> parallel scenario:
> write packing statistics:
> Packed 1 reqs - 448 times
> Packed 2 reqs - 38 times
> Packed 3 reqs - 23 times
> Packed 4 reqs - 30 times
> Packed 5 reqs - 14 times
> Packed 6 reqs - 8 times
> Packed 7 reqs - 4 times
> Packed 8 reqs - 1 times
> Packed 10 reqs - 1 times
> Packed 34 reqs - 1 times
> stopped packing due to the following reasons:
> 2 times: wrong data direction (meaning a READ was fetched and stopped the
> packing)
> 1 times: flush or discard
> 565 times: empty queue (meaning blk_fetch_request returned NULL)
>
>>
>> And I requested blktrace to confirm that this is indeed the behaviour.
>
> The trace logs show that in case of no packing, there are maximum of 3-4
> requests issued before a read request, while with packing there are also
> cases of 6 and 7 requests dispatched before a read request.
>
> I'm waiting for an approval for sharing the block trace logs.
> Since this is a simple test to run you can collect the trace logs and let
> us know if you reach other conclusions.
>
Thanks for the brief. I don't have the eMMC4.5 device with me yet, so
I can't reproduce the result. The problem you describe is most likely
applicable
to any block device driver with a large queue depth ( any queue depth >1).
I'll check to see what knobs in block affect the result.
Speaking of it, what is the host controller you use to test this ?
I was wondering if host->max_seg_size is taken into account while packed command
is in use. If not, shouldn't it be ?  - it could act as a better
throttle for "packing density".

Thanks,
Venkat.
