Path: news.gmane.org!not-for-mail
From: Peter Zijlstra <peterz@infradead.org>
Newsgroups: gmane.linux.kernel
Subject: Re: [PATCHSET] workqueue: reimplement CPU hotplug to keep idle
 workers
Date: Fri, 20 Jul 2012 19:21:17 +0200
Lines: 42
Approved: news@gmane.org
Message-ID: <1342804877.2583.42.camel@twins>
References: <1342545149-3515-1-git-send-email-tj@kernel.org>
	 <1342799311.2583.7.camel@twins> <20120720170255.GE32763@google.com>
NNTP-Posting-Host: plane.gmane.org
Mime-Version: 1.0
Content-Type: text/plain; charset=US-ASCII
Content-Transfer-Encoding: 7BIT
X-Trace: dough.gmane.org 1342804894 31923 80.91.229.3 (20 Jul 2012 17:21:34 GMT)
X-Complaints-To: usenet@dough.gmane.org
NNTP-Posting-Date: Fri, 20 Jul 2012 17:21:34 +0000 (UTC)
Cc: linux-kernel@vger.kernel.org, torvalds@linux-foundation.org,
	tglx@linutronix.de, linux-pm@vger.kernel.org
To: Tejun Heo <tj@kernel.org>
Original-X-From: linux-kernel-owner@vger.kernel.org Fri Jul 20 19:21:34 2012
Return-path: <linux-kernel-owner@vger.kernel.org>
Envelope-to: glk-linux-kernel-3@plane.gmane.org
Original-Received: from vger.kernel.org ([209.132.180.67])
	by plane.gmane.org with esmtp (Exim 4.69)
	(envelope-from <linux-kernel-owner@vger.kernel.org>)
	id 1SsGtR-0000uf-0O
	for glk-linux-kernel-3@plane.gmane.org; Fri, 20 Jul 2012 19:21:33 +0200
Original-Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753450Ab2GTRV0 (ORCPT <rfc822;glk-linux-kernel-3@m.gmane.org>);
	Fri, 20 Jul 2012 13:21:26 -0400
Original-Received: from merlin.infradead.org ([205.233.59.134]:53506 "EHLO
	merlin.infradead.org" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752793Ab2GTRVY convert rfc822-to-8bit (ORCPT
	<rfc822;linux-kernel@vger.kernel.org>);
	Fri, 20 Jul 2012 13:21:24 -0400
Original-Received: from canuck.infradead.org ([134.117.69.58])
	by merlin.infradead.org with esmtps (Exim 4.76 #1 (Red Hat Linux))
	id 1SsGtE-0003QG-VK; Fri, 20 Jul 2012 17:21:21 +0000
Original-Received: from dhcp-089-099-019-018.chello.nl ([89.99.19.18] helo=twins)
	by canuck.infradead.org with esmtpsa (Exim 4.76 #1 (Red Hat Linux))
	id 1SsGtD-0006g4-82; Fri, 20 Jul 2012 17:21:19 +0000
Original-Received: by twins (Postfix, from userid 1000)
	id 4A720800514C; Fri, 20 Jul 2012 19:21:17 +0200 (CEST)
In-Reply-To: <20120720170255.GE32763@google.com>
X-Mailer: Evolution 3.2.2- 
Original-Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
Xref: news.gmane.org gmane.linux.kernel:1330965
Archived-At: <http://permalink.gmane.org/gmane.linux.kernel/1330965>

On Fri, 2012-07-20 at 10:02 -0700, Tejun Heo wrote:
> Hey, Peter.
> 
> On Fri, Jul 20, 2012 at 05:48:31PM +0200, Peter Zijlstra wrote:
> > On Tue, 2012-07-17 at 10:12 -0700, Tejun Heo wrote:
> > > While this makes rebinding somewhat more complicated, as it has to be
> > > able to rebind idle workers too, it allows overall hotplug path to be
> > > much simpler.  
> > 
> > I really don't see the point of re-binding.. at that point you've well
> > and proper violated any per-cpu expectation, so why not complete running
> > the works on the disassociated thing and let new works accrue on the
> > per-cpu things again?
> 
> We've discussed this a couple times now, so the existing reasons were,
> 
> * Local affinity is more often used as a form of affinity optimization
>   since the beginning.  This, mixed with queue_work() /
>   queue_work_on(), does make things muddy.
> 
> * With local affinity used for optimization, we better support
>   detaching running workers - before cmwq, this used to be one of the
>   sources of trouble during power state changes.
> 
> * So, we have unbound workers which started as bound while a CPU is
>   down.  When the CPU comes back up again, we can do one of the
>   followings - 1. migrate the unbound ones to WORK_CPU_UNBOUND (can
>   also do this on CPU_DOWN), 2. leave them unbound and keep them
>   running in parallel with bound ones, or 3. rebind them.  #2 is the
>   hariest - it contaminates the usual !hotplug code paths.  #1 or #3,
>   unsure, but given how global_cwq's don't usually interact with each
>   other, I thought #3 would be lower impact on hot paths.
> 
> So, the above was my rationale before this "we need to stop destroying
> and re-creating kthreads across CPU hotplug events because phones do
> it gazillion times".  Now, I don't think we have any other way.

OK, so why can't you splice the list of works from the CPU going down
onto the list of the CPU doing the down and convert any busy worker
threads to be bound to the cpu doing down?

That way there's nothing 'left' to get back to on up.
