Path: news.gmane.org!not-for-mail
From: Hugh Dickins <hughd@google.com>
Newsgroups: gmane.linux.kernel.mm,gmane.linux.kernel,gmane.linux.kernel.cross-arch
Subject: Re: [RFC] page-table walkers vs memory order
Date: Tue, 24 Jul 2012 14:51:05 -0700 (PDT)
Lines: 169
Approved: news@gmane.org
Message-ID: <alpine.LSU.2.00.1207241356350.2094@eggly.anvils>
References: <1343064870.26034.23.camel@twins>
NNTP-Posting-Host: plane.gmane.org
Mime-Version: 1.0
Content-Type: TEXT/PLAIN; charset=US-ASCII
X-Trace: dough.gmane.org 1343166731 22234 80.91.229.3 (24 Jul 2012 21:52:11 GMT)
X-Complaints-To: usenet@dough.gmane.org
NNTP-Posting-Date: Tue, 24 Jul 2012 21:52:11 +0000 (UTC)
Cc: Linus Torvalds <torvalds@linux-foundation.org>, 
    "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>, 
    Rik van Riel <riel@redhat.com>, Andrew Morton <akpm@linux-foundation.org>, 
    Nick Piggin <npiggin@kernel.dk>, Andrea Arcangeli <aarcange@redhat.com>, 
    linux-kernel@vger.kernel.org, linux-arch@vger.kernel.org, 
    linux-mm@kvack.org
To: Peter Zijlstra <peterz@infradead.org>
Original-X-From: owner-linux-mm@kvack.org Tue Jul 24 23:52:07 2012
Return-path: <owner-linux-mm@kvack.org>
Envelope-to: glkm-linux-mm-2@m.gmane.org
Original-Received: from kanga.kvack.org ([205.233.56.17])
	by plane.gmane.org with esmtp (Exim 4.69)
	(envelope-from <owner-linux-mm@kvack.org>)
	id 1Stn1N-0006F2-EI
	for glkm-linux-mm-2@m.gmane.org; Tue, 24 Jul 2012 23:52:01 +0200
Original-Received: by kanga.kvack.org (Postfix)
	id 301C76B0044; Tue, 24 Jul 2012 17:51:58 -0400 (EDT)
Delivered-To: linux-mm-outgoing@kvack.org
Original-Received: by kanga.kvack.org (Postfix, from userid 40)
	id 2190B6B005A; Tue, 24 Jul 2012 17:51:58 -0400 (EDT)
X-Original-To: int-list-linux-mm@kvack.org
Delivered-To: int-list-linux-mm@kvack.org
Original-Received: by kanga.kvack.org (Postfix, from userid 63042)
	id DC6696B005D; Tue, 24 Jul 2012 17:51:57 -0400 (EDT)
X-Original-To: linux-mm@kvack.org
Delivered-To: linux-mm@kvack.org
Original-Received: from psmtp.com (na3sys010amx120.postini.com [74.125.245.120])
	by kanga.kvack.org (Postfix) with SMTP id A87686B0044
	for <linux-mm@kvack.org>; Tue, 24 Jul 2012 17:51:56 -0400 (EDT)
Original-Received: from mail-gh0-f169.google.com ([209.85.160.169]) (using TLSv1) by na3sys010amx120.postini.com ([74.125.244.10]) with SMTP;
	Tue, 24 Jul 2012 21:51:56 GMT
Original-Received: by ghrr18 with SMTP id r18so73948ghr.14
        for <linux-mm@kvack.org>; Tue, 24 Jul 2012 14:51:55 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=google.com; s=20120113;
        h=date:from:x-x-sender:to:cc:subject:in-reply-to:message-id
         :references:user-agent:mime-version:content-type;
        bh=/GaRNqrzucZvkPo0N46t6z13UgtswFT6PZu530xI7e8=;
        b=j8lMfR0uW2RAg5u/NSWG6/JATqd2rTh/22C/t2AKnjCvdcHq1CbcRiEHcDrengxI4l
         inNZcUeC+jbn7OxL7Cm/ulY3tu13Pg5cO4IhV4+OHfOIHBGMBcJ/EMWrnXjg53F8TDtX
         kIAtfgzYgz3vtiHE4cwDXP7gpPkjQlQI0bfDIPdg/9bT7VewRpt8cqDtyZ2+e9SnxztO
         wUzV8VBpqt/56UNvcbgHveLvkbS/ks+in2Y0IOTns9Bu9YbpGf/aqMRX1137YaWXmeHG
         hwegNSkRZhIGh3AYmuoHq+iqBCpknAS3CE/QNi1mdG3LsJU5KdvQbvEaZcg/Y1z/jgLs
         KCbw==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=google.com; s=20120113;
        h=date:from:x-x-sender:to:cc:subject:in-reply-to:message-id
         :references:user-agent:mime-version:content-type:x-gm-message-state;
        bh=/GaRNqrzucZvkPo0N46t6z13UgtswFT6PZu530xI7e8=;
        b=ckzv3cRvUDwnmQmD9Y6kYBxaGHDhQLEnjxbDcOBiE/T9Dk8hKw2qeYWotr1XZQGcrq
         7L37ZmOV8NfuRODLhTvZAUx/2oHEqTFLr75Lxbj6LJMRbVfVclklAt/f6KpDuq95AIxo
         4SLGEilemz68YWzDkptSkcuugojRITI6P6GnBSoviLrMg6IQ6cU2H93+jRtBS/RXcO4P
         uoLKiSWHVCe2FmH5LtAl9FDnjFEqcSdteJy2vpkyMQ5P3Fly/6noXMwX1IuqAxbsub58
         pSs7PcNz2+WAsqZHakA0rY0T2sCGs0UKaWm9xZX5nWLzhqhF7YCHsM6xNY6Myw/Oqx97
         NMcQ==
Original-Received: by 10.43.110.197 with SMTP id el5mr18678818icc.1.1343166715615;
        Tue, 24 Jul 2012 14:51:55 -0700 (PDT)
Original-Received: by 10.43.110.197 with SMTP id el5mr18678790icc.1.1343166715472;
        Tue, 24 Jul 2012 14:51:55 -0700 (PDT)
Original-Received: from [2620:0:1000:fd2e:224:d7ff:fee2:b75c] ([2620:0:1000:fd2e:224:d7ff:fee2:b75c])
        by mx.google.com with ESMTPS id z8sm3009543igi.5.2012.07.24.14.51.53
        (version=TLSv1/SSLv3 cipher=OTHER);
        Tue, 24 Jul 2012 14:51:53 -0700 (PDT)
X-X-Sender: hugh@eggly.anvils
In-Reply-To: <1343064870.26034.23.camel@twins>
User-Agent: Alpine 2.00 (LSU 1167 2008-08-23)
X-Gm-Message-State: ALoCoQks6Rzq1+z+L8w7fAsZJH8AEROpqdMi9jSwSLTdtBLeq63mnA4YtAyZhpewJ/oumevBzJmKRDhy5gldYjSPnDB1Q3RjMcpxvdaxx5MMZsqm7nNhn8cFsL8efpC6ZFuCqQXhJ9ZMjX9RKmfpM/oYbOU9UT+P7IBTIMMCQpg1f0KRqy2XGO7+wAxURd0218cBhw69QI2S
X-pstn-levels: (S:99.90000/99.90000 CV:99.9000 FC:95.5390 LC:95.5390 R:95.9108 P:95.9108 M:97.0282 C:98.6951 )
X-pstn-dkim: 1 skipped:not-enabled
X-pstn-settings: 3 (1.0000:0.1000) s cv GT3 gt2 gt1 r p m c 
X-pstn-addresses: from <hughd@google.com> [db-null] 
X-Bogosity: Ham, tests=bogofilter, spamicity=0.000000, version=1.2.2
Original-Sender: owner-linux-mm@kvack.org
Precedence: bulk
X-Loop: owner-majordomo@kvack.org
List-ID: <linux-mm.kvack.org>
Xref: news.gmane.org gmane.linux.kernel.mm:82957 gmane.linux.kernel:1332605 gmane.linux.kernel.cross-arch:14702
Archived-At: <http://permalink.gmane.org/gmane.linux.kernel/1332605>

On Mon, 23 Jul 2012, Peter Zijlstra wrote:
> 
> While staring at mm/huge_memory.c I found a very under-commented
> smp_wmb() in __split_huge_page_map(). It turns out that its copied from
> __{pte,pmd,pud}_alloc() but forgot the useful comment (or a reference
> thereto).
> 
> Now, afaict we're not good, as per that comment. Paul has since
> convinced some of us that compiler writers are pure evil and out to get
> us.
> 
> Therefore we should do what rcu_dereference() does and use
> ACCESS_ONCE()/barrier() followed smp_read_barrier_depends() every time
> we dereference a page-table pointer.
> 
> 
> In particular it looks like things like
> mm/memcontrol.c:mem_cgroup_count_precharge(), which use
> walk_page_range() under down_read(&mm->mmap_sem) and can thus be
> concurrent with __{pte,pmd,pud}_alloc() from faults (and possibly
> itself) are quite broken on Alpha

The Alpha pmd_offset() and pte_offset_map() already contain an
smp_read_barrier_depends() (362a61ad from Nick); with comment that
it's not needed on the pgd, and I presume the pud level is folded.
Does Alpha really need more of them, as you have put below?

> and subtly broken for those of us with 'creative' compilers.

I don't want to fight against ACCESS_ONCE() (or barrier(): that's
interesting, thank you, I hadn't seen it used as an ACCESS_ONCE()
substitute before); but I do want to question it a little.

I'm totally unclear whether the kernel ever gets built with these
'creative' compilers that you refer to.  Is ACCESS_ONCE() a warning
of where some future compiler would be permitted to mess with our
assumptions?  Or is it actually saving us already today?  Would we
know?  Could there be a boottime test that would tell us?  Is it
likely that a future compiler would have an "--access_once"
option that the kernel build would want to turn on?

Those may all be questions for Paul!

> 
> Should I go do a more extensive audit of page-table walkers or are we
> happy with the status quo?

I do love the status quo, but an audit would be welcome.  When
it comes to patches, personally I tend to prefer ACCESS_ONCE() and
smp_read_barrier_depends() and accompanying comments to be hidden away
in the underlying macros or inlines where reasonable, rather than
repeated all over; but I may have my priorities wrong on that.

The last time we rewrote the main pgd-pud-pmd-pte walkers,
we believed that no ACCESS_ONCE() was necessary, because although a
pgd-pud-pmd entry might be racily instantiated at any instant, it
could never change beneath us - the freeing of page tables happens
only when we cannot reach them by other routes.

(Never quite true: those _clear_bad() things can zero entries at any
instant, but we're already in a bad place when those come into play,
so we never worried about racing against them.)

Since then, I think THP has made the rules more complicated; but I
believe Andrea paid a great deal of attention to that kind of issue.

I suspect your arch/x86/mm/gup.c ACCESS_ONCE()s are necessary:
gup_fast() breaks as many rules as it can, and in particular may
be racing with the freeing of page tables; but I'm not so sure
about the pagewalk mods - we could say "cannot do any harm",
but I don't like adding lines on that basis.

Hugh

> 
> ---
>  arch/x86/mm/gup.c |    6 +++---
>  mm/pagewalk.c     |   24 ++++++++++++++++++++++++
>  2 files changed, 27 insertions(+), 3 deletions(-)
> 
> diff --git a/arch/x86/mm/gup.c b/arch/x86/mm/gup.c
> index dd74e46..4958fb1 100644
> --- a/arch/x86/mm/gup.c
> +++ b/arch/x86/mm/gup.c
> @@ -150,7 +150,7 @@ static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,
>  
>  	pmdp = pmd_offset(&pud, addr);
>  	do {
> -		pmd_t pmd = *pmdp;
> +		pmd_t pmd = ACCESS_ONCE(*pmdp);
>  
>  		next = pmd_addr_end(addr, end);
>  		/*
> @@ -220,7 +220,7 @@ static int gup_pud_range(pgd_t pgd, unsigned long addr, unsigned long end,
>  
>  	pudp = pud_offset(&pgd, addr);
>  	do {
> -		pud_t pud = *pudp;
> +		pud_t pud = ACCESS_ONCE(*pudp);
>  
>  		next = pud_addr_end(addr, end);
>  		if (pud_none(pud))
> @@ -280,7 +280,7 @@ int __get_user_pages_fast(unsigned long start, int nr_pages, int write,
>  	local_irq_save(flags);
>  	pgdp = pgd_offset(mm, addr);
>  	do {
> -		pgd_t pgd = *pgdp;
> +		pgd_t pgd = ACCESS_ONCE(*pgdp);
>  
>  		next = pgd_addr_end(addr, end);
>  		if (pgd_none(pgd))
> diff --git a/mm/pagewalk.c b/mm/pagewalk.c
> index 6c118d0..2ba2a74 100644
> --- a/mm/pagewalk.c
> +++ b/mm/pagewalk.c
> @@ -10,6 +10,14 @@ static int walk_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,
>  	int err = 0;
>  
>  	pte = pte_offset_map(pmd, addr);
> +	/*
> +	 * Pairs with the smp_wmb() in __{pte,pmd,pud}_alloc() and
> +	 * __split_huge_page_map(). Ideally we'd use ACCESS_ONCE() on the
> +	 * actual dereference of p[gum]d, but that's hidden deep within the
> +	 * bowels of {pte,pmd,pud}_offset.
> +	 */
> +	barrier();
> +	smp_read_barrier_depends();
>  	for (;;) {
>  		err = walk->pte_entry(pte, addr, addr + PAGE_SIZE, walk);
>  		if (err)
> @@ -32,6 +40,14 @@ static int walk_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,
>  	int err = 0;
>  
>  	pmd = pmd_offset(pud, addr);
> +	/*
> +	 * Pairs with the smp_wmb() in __{pte,pmd,pud}_alloc() and
> +	 * __split_huge_page_map(). Ideally we'd use ACCESS_ONCE() on the
> +	 * actual dereference of p[gum]d, but that's hidden deep within the
> +	 * bowels of {pte,pmd,pud}_offset.
> +	 */
> +	barrier();
> +	smp_read_barrier_depends();
>  	do {
>  again:
>  		next = pmd_addr_end(addr, end);
> @@ -77,6 +93,14 @@ static int walk_pud_range(pgd_t *pgd, unsigned long addr, unsigned long end,
>  	int err = 0;
>  
>  	pud = pud_offset(pgd, addr);
> +	/*
> +	 * Pairs with the smp_wmb() in __{pte,pmd,pud}_alloc() and
> +	 * __split_huge_page_map(). Ideally we'd use ACCESS_ONCE() on the
> +	 * actual dereference of p[gum]d, but that's hidden deep within the
> +	 * bowels of {pte,pmd,pud}_offset.
> +	 */
> +	barrier();
> +	smp_read_barrier_depends();
>  	do {
>  		next = pud_addr_end(addr, end);
>  		if (pud_none_or_clear_bad(pud)) {
> 
> 

--
To unsubscribe, send a message with 'unsubscribe linux-mm' in
the body to majordomo@kvack.org.  For more info on Linux MM,
see: http://www.linux-mm.org/ .
Don't email: <a href=mailto:"dont@kvack.org"> email@kvack.org </a>

