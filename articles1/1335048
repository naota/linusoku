Path: news.gmane.org!not-for-mail
From: Glauber Costa <glommer@parallels.com>
Newsgroups: gmane.linux.kernel.mm,gmane.linux.kernel,gmane.linux.openvz.devel,gmane.linux.kernel.cgroups
Subject: Re: [PATCH 06/10] sl[au]b: Allocate objects from memcg cache
Date: Mon, 30 Jul 2012 17:11:06 +0400
Lines: 86
Approved: news@gmane.org
Message-ID: <501687EA.5050401@parallels.com>
References: <1343227101-14217-1-git-send-email-glommer@parallels.com> <1343227101-14217-7-git-send-email-glommer@parallels.com> <20120730125851.GB27293@shutemov.name>
NNTP-Posting-Host: plane.gmane.org
Mime-Version: 1.0
Content-Type: text/plain; charset="ISO-8859-1"
Content-Transfer-Encoding: 7bit
X-Trace: dough.gmane.org 1343653892 8046 80.91.229.3 (30 Jul 2012 13:11:32 GMT)
X-Complaints-To: usenet@dough.gmane.org
NNTP-Posting-Date: Mon, 30 Jul 2012 13:11:32 +0000 (UTC)
Cc: <linux-kernel@vger.kernel.org>, <linux-mm@kvack.org>, Andrew Morton
	<akpm@linux-foundation.org>, Christoph Lameter <cl@linux.com>, David Rientjes
	<rientjes@google.com>, Pekka Enberg <penberg@kernel.org>, Greg Thelen
	<gthelen@google.com>, Johannes Weiner <hannes@cmpxchg.org>, Michal Hocko
	<mhocko@suse.cz>, Frederic Weisbecker <fweisbec@gmail.com>,
	<devel@openvz.org>, <cgroups@vger.kernel.org>, Pekka Enberg
	<penberg@cs.helsinki.fi>, Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>,
	Suleiman Souhlal <suleiman@google.com>
To: "Kirill A. Shutemov" <kirill@shutemov.name>
Original-X-From: owner-linux-mm@kvack.org Mon Jul 30 15:11:26 2012
Return-path: <owner-linux-mm@kvack.org>
Envelope-to: glkm-linux-mm-2@m.gmane.org
Original-Received: from kanga.kvack.org ([205.233.56.17])
	by plane.gmane.org with esmtp (Exim 4.69)
	(envelope-from <owner-linux-mm@kvack.org>)
	id 1Svpkr-0004u1-Ma
	for glkm-linux-mm-2@m.gmane.org; Mon, 30 Jul 2012 15:11:25 +0200
Original-Received: by kanga.kvack.org (Postfix)
	id 0E13C6B0068; Mon, 30 Jul 2012 09:11:24 -0400 (EDT)
Delivered-To: linux-mm-outgoing@kvack.org
Original-Received: by kanga.kvack.org (Postfix, from userid 40)
	id DD2D36B0069; Mon, 30 Jul 2012 09:11:23 -0400 (EDT)
X-Original-To: int-list-linux-mm@kvack.org
Delivered-To: int-list-linux-mm@kvack.org
Original-Received: by kanga.kvack.org (Postfix, from userid 63042)
	id AC2636B006C; Mon, 30 Jul 2012 09:11:23 -0400 (EDT)
X-Original-To: linux-mm@kvack.org
Delivered-To: linux-mm@kvack.org
Original-Received: from psmtp.com (na3sys010amx116.postini.com [74.125.245.116])
	by kanga.kvack.org (Postfix) with SMTP id E29EF6B0068
	for <linux-mm@kvack.org>; Mon, 30 Jul 2012 09:11:22 -0400 (EDT)
Original-Received: from mx2.parallels.com ([64.131.90.16]) (using TLSv1) by na3sys010amx116.postini.com ([74.125.244.10]) with SMTP;
	Mon, 30 Jul 2012 13:11:23 GMT
Original-Received: from [199.115.105.252] (helo=mail.parallels.com)
	by mx2.parallels.com with esmtps (TLSv1:AES128-SHA:128)
	(Exim 4.77)
	(envelope-from <glommer@parallels.com>)
	id 1Svpkf-0008W8-Ss; Mon, 30 Jul 2012 09:11:14 -0400
Original-Received: from straightjacket.localdomain (195.214.232.10) by
 mail.parallels.com (10.255.249.32) with Microsoft SMTP Server (TLS) id
 14.2.247.3; Mon, 30 Jul 2012 06:11:12 -0700
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:14.0) Gecko/20120717 Thunderbird/14.0
In-Reply-To: <20120730125851.GB27293@shutemov.name>
X-pstn-neptune: 0/0/0.00/0
X-pstn-levels: (S:99.90000/99.90000 CV:99.9000 FC:95.5390 LC:95.5390 R:95.9108 P:95.9108 M:97.0282 C:98.6951 )
X-pstn-dkim: 0 skipped:not-enabled
X-pstn-settings: 3 (1.0000:1.0000) s cv gt3 gt2 gt1 r p m c 
X-pstn-addresses: from <glommer@parallels.com> [db-null] 
X-Bogosity: Ham, tests=bogofilter, spamicity=0.000000, version=1.2.2
Original-Sender: owner-linux-mm@kvack.org
Precedence: bulk
X-Loop: owner-majordomo@kvack.org
List-ID: <linux-mm.kvack.org>
Xref: news.gmane.org gmane.linux.kernel.mm:83169 gmane.linux.kernel:1335048 gmane.linux.openvz.devel:1945 gmane.linux.kernel.cgroups:3482
Archived-At: <http://permalink.gmane.org/gmane.linux.kernel/1335048>

On 07/30/2012 04:58 PM, Kirill A. Shutemov wrote:
> On Wed, Jul 25, 2012 at 06:38:17PM +0400, Glauber Costa wrote:
>> We are able to match a cache allocation to a particular memcg.  If the
>> task doesn't change groups during the allocation itself - a rare event,
>> this will give us a good picture about who is the first group to touch a
>> cache page.
>>
>> This patch uses the now available infrastructure by calling
>> memcg_kmem_get_cache() before all the cache allocations.
>>
>> Signed-off-by: Glauber Costa <glommer@parallels.com>
>> CC: Christoph Lameter <cl@linux.com>
>> CC: Pekka Enberg <penberg@cs.helsinki.fi>
>> CC: Michal Hocko <mhocko@suse.cz>
>> CC: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
>> CC: Johannes Weiner <hannes@cmpxchg.org>
>> CC: Suleiman Souhlal <suleiman@google.com>
>> ---
>>  include/linux/slub_def.h |   18 +++++++++++++-----
>>  mm/memcontrol.c          |    2 ++
>>  mm/slab.c                |    4 ++++
>>  mm/slub.c                |    1 +
>>  4 files changed, 20 insertions(+), 5 deletions(-)
>>
>> diff --git a/include/linux/slub_def.h b/include/linux/slub_def.h
>> index 8bb8ad2..148000a 100644
>> --- a/include/linux/slub_def.h
>> +++ b/include/linux/slub_def.h
>> @@ -13,6 +13,8 @@
>>  #include <linux/kobject.h>
>>  
>>  #include <linux/kmemleak.h>
>> +#include <linux/memcontrol.h>
>> +#include <linux/mm.h>
>>  
>>  enum stat_item {
>>  	ALLOC_FASTPATH,		/* Allocation from cpu slab */
>> @@ -209,14 +211,14 @@ static __always_inline int kmalloc_index(size_t size)
>>   * This ought to end up with a global pointer to the right cache
>>   * in kmalloc_caches.
>>   */
>> -static __always_inline struct kmem_cache *kmalloc_slab(size_t size)
>> +static __always_inline struct kmem_cache *kmalloc_slab(gfp_t flags, size_t size)
>>  {
>>  	int index = kmalloc_index(size);
>>  
>>  	if (index == 0)
>>  		return NULL;
>>  
>> -	return kmalloc_caches[index];
>> +	return memcg_kmem_get_cache(kmalloc_caches[index], flags);
>>  }
>>  
>>  void *kmem_cache_alloc(struct kmem_cache *, gfp_t);
>> @@ -225,7 +227,13 @@ void *__kmalloc(size_t size, gfp_t flags);
>>  static __always_inline void *
>>  kmalloc_order(size_t size, gfp_t flags, unsigned int order)
>>  {
>> -	void *ret = (void *) __get_free_pages(flags | __GFP_COMP, order);
>> +	void *ret;
>> +
>> +	flags = __GFP_COMP;
>> +#ifdef CONFIG_MEMCG_KMEM
>> +	flags |= __GFP_KMEMCG;
>> +#endif
> 
> Em.. I don't see where __GFP_KMEMCG is defined.
> It should be 0 for !CONFIG_MEMCG_KMEM.
> 
It is not, sorry.

As I said, this is dependent on another patch series.
My main goal while sending this was to get the slab part - that will
eventually come ontop of that - discussed. Because they are both quite
complex, I believe they benefit from being discussed separately.

You can find the latest version of that here:

https://lkml.org/lkml/2012/6/25/251

--
To unsubscribe, send a message with 'unsubscribe linux-mm' in
the body to majordomo@kvack.org.  For more info on Linux MM,
see: http://www.linux-mm.org/ .
Don't email: <a href=mailto:"dont@kvack.org"> email@kvack.org </a>

