Path: news.gmane.org!not-for-mail
From: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Newsgroups: gmane.linux.kernel.mm,gmane.linux.kernel
Subject: Re: [PATCH] Cgroup: Fix memory accounting scalability in shrink_page_list
Date: Fri, 20 Jul 2012 12:19:20 +0900
Lines: 84
Approved: news@gmane.org
Message-ID: <5008CE38.2020300@jp.fujitsu.com>
References: <1342740866.13492.50.camel@schen9-DESK>
NNTP-Posting-Host: plane.gmane.org
Mime-Version: 1.0
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Trace: dough.gmane.org 1342754528 1361 80.91.229.3 (20 Jul 2012 03:22:08 GMT)
X-Complaints-To: usenet@dough.gmane.org
NNTP-Posting-Date: Fri, 20 Jul 2012 03:22:08 +0000 (UTC)
Cc: Andrew Morton <akpm@linux-foundation.org>, Mel Gorman <mel@csn.ul.ie>, 
 Minchan Kim <minchan@kernel.org>,
 Johannes Weiner <hannes@cmpxchg.org>, 
 "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>,
 "andi.kleen" <andi.kleen@intel.com>, linux-mm <linux-mm@kvack.org>, 
 linux-kernel@vger.kernel.org
To: Tim Chen <tim.c.chen@linux.intel.com>
Original-X-From: owner-linux-mm@kvack.org Fri Jul 20 05:22:06 2012
Return-path: <owner-linux-mm@kvack.org>
Envelope-to: glkm-linux-mm-2@m.gmane.org
Original-Received: from kanga.kvack.org ([205.233.56.17])
	by plane.gmane.org with esmtp (Exim 4.69)
	(envelope-from <owner-linux-mm@kvack.org>)
	id 1Ss3n4-000767-9z
	for glkm-linux-mm-2@m.gmane.org; Fri, 20 Jul 2012 05:22:06 +0200
Original-Received: by kanga.kvack.org (Postfix)
	id 7AF7B6B004D; Thu, 19 Jul 2012 23:22:03 -0400 (EDT)
Delivered-To: linux-mm-outgoing@kvack.org
Original-Received: by kanga.kvack.org (Postfix, from userid 40)
	id 6A6036B005D; Thu, 19 Jul 2012 23:22:03 -0400 (EDT)
X-Original-To: int-list-linux-mm@kvack.org
Delivered-To: int-list-linux-mm@kvack.org
Original-Received: by kanga.kvack.org (Postfix, from userid 63042)
	id 333996B0068; Thu, 19 Jul 2012 23:22:03 -0400 (EDT)
X-Original-To: linux-mm@kvack.org
Delivered-To: linux-mm@kvack.org
Original-Received: from psmtp.com (na3sys010amx135.postini.com [74.125.245.135])
	by kanga.kvack.org (Postfix) with SMTP id 54B476B004D
	for <linux-mm@kvack.org>; Thu, 19 Jul 2012 23:22:02 -0400 (EDT)
Original-Received: from fgwmail6.fujitsu.co.jp ([192.51.44.36]) (using TLSv1) by na3sys010amx135.postini.com ([74.125.244.10]) with SMTP;
	Fri, 20 Jul 2012 03:22:02 GMT
Original-Received: from m2.gw.fujitsu.co.jp (unknown [10.0.50.72])
	by fgwmail6.fujitsu.co.jp (Postfix) with ESMTP id 1716D3EE0B5
	for <linux-mm@kvack.org>; Fri, 20 Jul 2012 12:22:00 +0900 (JST)
Original-Received: from smail (m2 [127.0.0.1])
	by outgoing.m2.gw.fujitsu.co.jp (Postfix) with ESMTP id EF37745DE4E
	for <linux-mm@kvack.org>; Fri, 20 Jul 2012 12:21:59 +0900 (JST)
Original-Received: from s2.gw.fujitsu.co.jp (s2.gw.fujitsu.co.jp [10.0.50.92])
	by m2.gw.fujitsu.co.jp (Postfix) with ESMTP id CCEA345DE4D
	for <linux-mm@kvack.org>; Fri, 20 Jul 2012 12:21:59 +0900 (JST)
Original-Received: from s2.gw.fujitsu.co.jp (localhost.localdomain [127.0.0.1])
	by s2.gw.fujitsu.co.jp (Postfix) with ESMTP id BB1391DB803C
	for <linux-mm@kvack.org>; Fri, 20 Jul 2012 12:21:59 +0900 (JST)
Original-Received: from m1001.s.css.fujitsu.com (m1001.s.css.fujitsu.com [10.240.81.139])
	by s2.gw.fujitsu.co.jp (Postfix) with ESMTP id 659771DB802C
	for <linux-mm@kvack.org>; Fri, 20 Jul 2012 12:21:59 +0900 (JST)
Original-Received: from m1001.css.fujitsu.com (m1001 [127.0.0.1])
	by m1001.s.css.fujitsu.com (Postfix) with ESMTP id 3EAB160DC4;
	Fri, 20 Jul 2012 12:21:59 +0900 (JST)
Original-Received: from [127.0.0.1] (unknown [10.124.101.152])
	by m1001.s.css.fujitsu.com (Postfix) with ESMTP id D25A860C0A;
	Fri, 20 Jul 2012 12:21:58 +0900 (JST)
X-SecurityPolicyCheck: OK by SHieldMailChecker v1.7.4
User-Agent: Mozilla/5.0 (Windows NT 6.0; rv:13.0) Gecko/20120614 Thunderbird/13.0.1
In-Reply-To: <1342740866.13492.50.camel@schen9-DESK>
X-pstn-neptune: 0/0/0.00/0
X-pstn-levels: (S:99.49286/99.90000 CV:99.9000 FC:95.5390 LC:95.5390 R:95.9108 P:95.9108 M:97.0282 C:98.6951 )
X-pstn-dkim: 0 skipped:not-enabled
X-pstn-settings: 3 (1.0000:1.0000) s cv gt3 gt2 gt1 r p m c 
X-pstn-addresses: from <kamezawa.hiroyu@jp.fujitsu.com> [db-null] 
X-Bogosity: Ham, tests=bogofilter, spamicity=0.000000, version=1.2.2
Original-Sender: owner-linux-mm@kvack.org
Precedence: bulk
X-Loop: owner-majordomo@kvack.org
List-ID: <linux-mm.kvack.org>
Xref: news.gmane.org gmane.linux.kernel.mm:82738 gmane.linux.kernel:1330650
Archived-At: <http://permalink.gmane.org/gmane.linux.kernel/1330650>

(2012/07/20 8:34), Tim Chen wrote:
> Hi,
>
> I noticed in a multi-process parallel files reading benchmark I ran on a
> 8 socket machine,  throughput slowed down by a factor of 8 when I ran
> the benchmark within a cgroup container.  I traced the problem to the
> following code path (see below) when we are trying to reclaim memory
> from file cache.  The res_counter_uncharge function is called on every
> page that's reclaimed and created heavy lock contention.  The patch
> below allows the reclaimed pages to be uncharged from the resource
> counter in batch and recovered the regression.
>
> Tim
>
>       40.67%           usemem  [kernel.kallsyms]                   [k] _raw_spin_lock
>                        |
>                        --- _raw_spin_lock
>                           |
>                           |--92.61%-- res_counter_uncharge
>                           |          |
>                           |          |--100.00%-- __mem_cgroup_uncharge_common
>                           |          |          |
>                           |          |          |--100.00%-- mem_cgroup_uncharge_cache_page
>                           |          |          |          __remove_mapping
>                           |          |          |          shrink_page_list
>                           |          |          |          shrink_inactive_list
>                           |          |          |          shrink_mem_cgroup_zone
>                           |          |          |          shrink_zone
>                           |          |          |          do_try_to_free_pages
>                           |          |          |          try_to_free_pages
>                           |          |          |          __alloc_pages_nodemask
>                           |          |          |          alloc_pages_current
>
>

Thank you very much !!

When I added batching, I didn't touch page-reclaim path because it delays
res_counter_uncharge() and make more threads run into page reclaim.
But, from above score, bactching seems required.

And because of current design of per-zone-per-memcg-LRU, batching
works very very well....all lru pages shrink_page_list() scans are on
the same memcg.

BTW, it's better to show 'how much improved' in patch description..


> ---
> Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
> diff --git a/mm/vmscan.c b/mm/vmscan.c
> index 33dc256..aac5672 100644
> --- a/mm/vmscan.c
> +++ b/mm/vmscan.c
> @@ -779,6 +779,7 @@ static unsigned long shrink_page_list(struct list_head *page_list,
>
>   	cond_resched();
>
> +	mem_cgroup_uncharge_start();
>   	while (!list_empty(page_list)) {
>   		enum page_references references;
>   		struct address_space *mapping;
> @@ -1026,6 +1027,7 @@ keep_lumpy:
>
>   	list_splice(&ret_pages, page_list);
>   	count_vm_events(PGACTIVATE, pgactivate);
> +	mem_cgroup_uncharge_end();

I guess placing mem_cgroup_uncharge_end() just after the loop may be better looking.

Anyway,
Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>

But please show 'how much improved' in patch description.

Thanks,
-Kame

--
To unsubscribe, send a message with 'unsubscribe linux-mm' in
the body to majordomo@kvack.org.  For more info on Linux MM,
see: http://www.linux-mm.org/ .
Don't email: <a href=mailto:"dont@kvack.org"> email@kvack.org </a>

