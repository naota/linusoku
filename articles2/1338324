Path: news.gmane.org!not-for-mail
From: Pedro Larroy <pedro.larroy.lists@gmail.com>
Newsgroups: gmane.linux.kernel
Subject: Re: unfair scheduling with tbb application observed, could it be a
 kernel issue?
Date: Mon, 6 Aug 2012 16:28:55 +0200
Lines: 46
Approved: news@gmane.org
Message-ID: <CAC_CU1iP0y9pVPPWVuER3kZbFGbBwS6Q2w1mJ6Ke6VNLqJOgbA@mail.gmail.com>
References: <CAC_CU1hzuSAeHG7-5g5kOa=MMh3KnJZNwWr+ySkJQyxixck4WA@mail.gmail.com>
	<1344263139.6853.10.camel@marge.simpson.net>
NNTP-Posting-Host: plane.gmane.org
Mime-Version: 1.0
Content-Type: text/plain; charset=ISO-8859-1
X-Trace: dough.gmane.org 1344263349 8173 80.91.229.3 (6 Aug 2012 14:29:09 GMT)
X-Complaints-To: usenet@dough.gmane.org
NNTP-Posting-Date: Mon, 6 Aug 2012 14:29:09 +0000 (UTC)
Cc: linux-kernel@vger.kernel.org
To: Mike Galbraith <efault@gmx.de>
Original-X-From: linux-kernel-owner@vger.kernel.org Mon Aug 06 16:29:08 2012
Return-path: <linux-kernel-owner@vger.kernel.org>
Envelope-to: glk-linux-kernel-3@plane.gmane.org
Original-Received: from vger.kernel.org ([209.132.180.67])
	by plane.gmane.org with esmtp (Exim 4.69)
	(envelope-from <linux-kernel-owner@vger.kernel.org>)
	id 1SyOIr-0002v0-Vl
	for glk-linux-kernel-3@plane.gmane.org; Mon, 06 Aug 2012 16:29:06 +0200
Original-Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1756552Ab2HFO25 (ORCPT <rfc822;glk-linux-kernel-3@m.gmane.org>);
	Mon, 6 Aug 2012 10:28:57 -0400
Original-Received: from mail-pb0-f46.google.com ([209.85.160.46]:62840 "EHLO
	mail-pb0-f46.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1756493Ab2HFO24 (ORCPT
	<rfc822;linux-kernel@vger.kernel.org>);
	Mon, 6 Aug 2012 10:28:56 -0400
Original-Received: by pbbrr13 with SMTP id rr13so2658158pbb.19
        for <linux-kernel@vger.kernel.org>; Mon, 06 Aug 2012 07:28:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=c4yvdUNqzpCYT6Fxp0BQsuYAm/JWslWTuRQYuTjYl60=;
        b=hq8ZH1mUA88EEh2zNgBwZXE6uXzox7HsoA1Fq4jsQQuQMMj7jJ7p03ay556ZTXqT5h
         UlgQJWQj2f8Fi5+YFM7ZSmfJKsoSsEbg82fbodxKFVf1H3vKup1IFJ06wRKz1HM+/n5h
         InJRPRTS4PwcCHW7wzda2reXRHkkjnZI1oXdTAEF6aIJpBudhwAGxS/db8nyqyBHSecW
         nsV85CQQ08ttsYHPE5IOFsU15sQel+6rtMV8l7ovUUKG3pHdEiNFjiClSw+fAebM3jYb
         VKJ/fnBVp6DUe21zhIeZL8xUXCHCO72KOdEoumpUFvCeU/dWLOZcd/Xrwzh55bKZj18w
         Wprw==
Original-Received: by 10.68.224.39 with SMTP id qz7mr19352303pbc.127.1344263336072;
 Mon, 06 Aug 2012 07:28:56 -0700 (PDT)
Original-Received: by 10.68.4.10 with HTTP; Mon, 6 Aug 2012 07:28:55 -0700 (PDT)
In-Reply-To: <1344263139.6853.10.camel@marge.simpson.net>
Original-Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
Xref: news.gmane.org gmane.linux.kernel:1338324
Archived-At: <http://permalink.gmane.org/gmane.linux.kernel/1338324>

I didn't enable cgroups explicitly, and don't have userspace tools for
that installed.

Thanks.

Pedro.

On Mon, Aug 6, 2012 at 4:25 PM, Mike Galbraith <efault@gmx.de> wrote:
> On Mon, 2012-08-06 at 16:04 +0200, Pedro Larroy wrote:
>> Hi
>>
>> I think we are observing unfair scheduling of processes that use intel
>> TBB thread scheduler, as we have several processes with nice of 19 and
>> ioniced idle, and somehow the process with nice 0 should be getting
>> more than 1000% cpu
> ..
>> Tasks: 559 total,  37 running, 522 sleeping,   0 stopped,   0 zombie
>> Cpu(s): 67.8%us, 16.0%sy, 13.0%ni,  1.7%id,  0.6%wa,  0.0%hi,  1.0%si,  0.0%st
>> Mem:  98998032k total, 97444688k used,  1553344k free,    53772k buffers
>> Swap:  4198316k total,   704860k used,  3493456k free, 73270776k cached
>>
>>   PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
>> 14373 disco     39  19 8734m 6.9g  12m R  107  7.3  36:09.72 mmcc
>> 15293 disco     39  19 3174m 1.4g  12m R  101  1.5  19:33.79 mmcc
>> 20341 disco     39  19 2735m 1.1g  12m R  101  1.1   8:40.38 mmcc
>> 18241 disco     39  19 3040m 1.3g  11m R  100  1.4  14:27.91 mmcc
>> 15204 disco     39  19 5418m 3.7g  12m R   99  3.9  20:53.89 mmcc
>> 24901 larroy    20   0  327m 296m 4276 R   88  0.3   0:04.14 cc1plus
>> 24942 larroy    20   0  193m 159m 4008 R   87  0.2   0:01.47 cc1plus
>> 24862 larroy    20   0  417m 388m 7992 R   84  0.4   0:07.02 cc1plus
>> 24959 larroy    20   0  184m 153m 4008 R   80  0.2   0:01.32 cc1plus
>> 24935 larroy    20   0  254m 222m 4024 R   77  0.2   0:02.44 cc1plus
>> 24919 larroy    20   0  336m 301m 4036 R   76  0.3   0:03.61 cc1plus
>> 24972 larroy    20   0 43160  15m 2332 R   76  0.0   0:00.95 cc1plus
>> 24918 larroy    20   0  287m 255m 4024 R   70  0.3   0:02.99 cc1plus
>> 24962 larroy    20   0 44872  17m 2332 R   69  0.0   0:01.23 cc1plus
>> 24976 larroy    20   0 41212  14m 2332 R   66  0.0   0:00.67 cc1plus
>> 24501 larroy    20   0  687m 657m 8044 R   64  0.7   0:22.97 cc1plus
>> 24933 larroy    20   0  211m 177m 4008 R   62  0.2   0:01.79 cc1plus
>> 24899 larroy    20   0  327m 296m 4276 R   57  0.3   0:04.25 cc1plus
>
> Are tasks running in per user cgroups or such?  If so, you'd need to
> adjust group shares.
>
> -Mike
>
